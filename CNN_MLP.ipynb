{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc10c48e",
   "metadata": {},
   "source": [
    "# Q2\n",
    "We are tasked with implementing a model called \"CNN-MLP\" proposed by the authors of the 2020 paper called \"Remote sensing image scene classification using CNN-MLP with data augmentation\" which aims to improve classification accuracy on VHR imagery. In this assignment we will use one of the 3 used publicly-available datasets used in the paper called \"UC-Merced\" dataset. We expand our training dataset via data augmentation and modify the pre-trained Xception network to create the \"CNN-MLP\" model. Lastly we will evaluate the performance of the obtained model to verify the effectiveness of \"CNN_MLP\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ae4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ucmerced_augmentations():\n",
    "    \"\"\"Return list of augmentation transforms and their names.\"\"\"\n",
    "    augmentation_transforms = [\n",
    "        transforms.RandomRotation(degrees=(90,91)),           # 90° rotation\n",
    "        transforms.RandomRotation(degrees=(180,181)),        # 180° rotation\n",
    "        transforms.RandomRotation(degrees=(270,271)),        # 270° rotation\n",
    "        transforms.RandomHorizontalFlip(p=1.0),              # Horizontal flip\n",
    "        transforms.RandomVerticalFlip(p=1.0),                # Vertical flip\n",
    "        transforms.RandomResizedCrop(299, scale=(0.8, 1.0)) # Zoom + scale\n",
    "    ]\n",
    "    \n",
    "    augmentation_names = [\n",
    "        \"Original\",\n",
    "        \"Rotate 90°\",\n",
    "        \"Rotate 180°\", \n",
    "        \"Rotate 270°\",\n",
    "        \"Horizontal Flip\",\n",
    "        \"Vertical Flip\",\n",
    "        \"Zoom 80-100%\"\n",
    "    ]\n",
    "    return augmentation_transforms, augmentation_names\n",
    "\n",
    "\n",
    "def apply_augmentations(image, augmentations):\n",
    "    augmented_images = [image]\n",
    "    for transform in augmentations:\n",
    "        try:\n",
    "            augmented_img = transform(image)\n",
    "            augmented_img = augmented_img.resize(image.size, Image.BILINEAR)\n",
    "            augmented_images.append(augmented_img)\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation failed: {e}, using original\")\n",
    "            augmented_images.append(image)\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be038e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ucmerced_dataset(image_paths, labels, transform=None, augment=False):\n",
    "    dataset = []\n",
    "    augmentations, _ = get_ucmerced_augmentations() if augment else ([], [])\n",
    "    \n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        images_to_add = [image]\n",
    "        if augment:\n",
    "            images_to_add = apply_augmentations(image, augmentations)\n",
    "        \n",
    "        for img in images_to_add:\n",
    "            if transform:\n",
    "                img = transform(img)\n",
    "            dataset.append((img, label))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_augmented_samples(dataset, num_samples=1):\n",
    "    \"\"\"Display original and augmented samples with proper names\"\"\"\n",
    "    print(\"\\nShowing augmented samples...\")\n",
    "    \n",
    "    aug_names = [\n",
    "        \"Original\",\n",
    "        \"Rotate 90°\", \n",
    "        \"Rotate 180°\",\n",
    "        \"Rotate 270°\",\n",
    "        \"Horizontal Flip\",\n",
    "        \"Vertical Flip\",\n",
    "        \"Zoom 80-100%\"\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 7, figsize=(21, 3*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        orig_idx = i * 7\n",
    "        \n",
    "        for j in range(7):\n",
    "            try:\n",
    "                img, label = dataset[orig_idx + j]\n",
    "                \n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(f'{aug_names[j]}', fontsize=10)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "            except IndexError:\n",
    "                axes[i, j].axis('off')\n",
    "                axes[i, j].set_title('N/A', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Original and Augmented Images', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_features, num_classes, hidden_size=74, dropout_rate=0.4):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        \n",
    "        self.input_features = input_features\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_norm = nn.BatchNorm1d(input_features)\n",
    "        self.fc1 = nn.Linear(input_features, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # First layer with sigmoid\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e01b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, feature_dim=2048, mlp_hidden_size=74, dropout_rate=0.4):\n",
    "        super(CNNMLP, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = timm.create_model('xception', pretrained=True, num_classes=0, global_pool='avg')\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        self.classifier = EnhancedMLP(\n",
    "            input_features=feature_dim,\n",
    "            num_classes=num_classes,\n",
    "            hidden_size=mlp_hidden_size,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        self._freeze_feature_extractor()\n",
    "    \n",
    "    def _freeze_feature_extractor(self):\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(x)\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_mlp_trainer(model, num_classes, device='cuda' if torch.cuda.is_available() else 'cpu', opt=\"Adagrad\"):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if opt == \"Adagrad\":\n",
    "        optimizer = optim.Adagrad(\n",
    "            model.classifier.parameters(),\n",
    "            lr=0.01\n",
    "        )\n",
    "    elif opt == \"Adam\":\n",
    "        optimizer = optim.Adam(\n",
    "            model.classifier.parameters(),\n",
    "            lr=0.01,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': criterion,\n",
    "        'device': device,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }\n",
    "\n",
    "def train_mlp_classifier(trainer_data, train_loader, test_loader, epochs=5):\n",
    "    model = trainer_data[\"model\"]\n",
    "    criterion = trainer_data[\"criterion\"]\n",
    "    optimizer = trainer_data[\"optimizer\"]\n",
    "    device = trainer_data[\"device\"]\n",
    "\n",
    "    train_losses = trainer_data[\"train_losses\"]\n",
    "    train_accuracies = trainer_data[\"train_accuracies\"]\n",
    "    test_accuracies = trainer_data[\"test_accuracies\"]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * correct / total\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "        epoch_test_acc = test_model(trainer_data, test_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Loss: {epoch_loss:.2f} | \"\n",
    "              f\"Train Acc: {epoch_train_acc:.2f}% | \"\n",
    "              f\"Test Acc: {epoch_test_acc:.2f}%\")\n",
    "\n",
    "    return trainer_data\n",
    "\n",
    "\n",
    "\n",
    "def test_model(trainer, test_loader):\n",
    "    trainer['model'].eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(trainer['device']), target.to(trainer['device'])\n",
    "            output = trainer['model'](data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    trainer['test_accuracies'].append(accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def calculate_class_accuracy(trainer, data_loader):\n",
    "    trainer['model'].eval()\n",
    "    class_correct = {}\n",
    "    class_total = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(trainer['device']), target.to(trainer['device'])\n",
    "            output = trainer['model'](data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            for label, prediction in zip(target, predicted):\n",
    "                label = label.item()\n",
    "                if label not in class_correct:\n",
    "                    class_correct[label] = 0\n",
    "                    class_total[label] = 0\n",
    "                \n",
    "                class_total[label] += 1\n",
    "                if label == prediction.item():\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    class_accuracy = {}\n",
    "    for label in class_total:\n",
    "        class_accuracy[label] = 100 * class_correct[label] / class_total[label]\n",
    "    \n",
    "    total_accuracy = 100 * sum(class_correct.values()) / sum(class_total.values())\n",
    "    \n",
    "    return class_accuracy, total_accuracy\n",
    "\n",
    "def get_training_history(trainer):\n",
    "    return {\n",
    "        'train_losses': trainer['train_losses'],\n",
    "        'train_accuracies': trainer['train_accuracies'],\n",
    "        'test_accuracies': trainer['test_accuracies']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    #Loss\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(history['train_losses'], label='Train Loss', color='red')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    #Train Acc\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(history['train_accuracies'], label='Train Accuracy', color='blue')\n",
    "    plt.title(\"Train Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    #Test Acc\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(history['test_accuracies'], label='Test Accuracy', color='green')\n",
    "    plt.title(\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0a266",
   "metadata": {},
   "source": [
    "### 2-1 Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04aba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'UCMerced_LandUse/Images'\n",
    "\n",
    "class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "for cls_name in class_names:\n",
    "    cls_dir = os.path.join(data_dir, cls_name)\n",
    "    for img_path in glob.glob(os.path.join(cls_dir, \"*.tif\")):\n",
    "        all_image_paths.append(img_path)\n",
    "        all_labels.append(class_to_idx[cls_name])\n",
    "\n",
    "print(\"Total images:\", len(all_image_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eead464",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    all_image_paths, all_labels,\n",
    "    train_size=0.8,\n",
    "    stratify=all_labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "train_set = set(train_paths)\n",
    "test_set = set(test_paths)\n",
    "overlap = train_set.intersection(test_set)\n",
    "print(f\"Overlapping images between train and test: {len(overlap)}\")\n",
    "assert len(overlap) == 0, \"Data leakage detected: train and test sets have overlapping images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04101e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = load_ucmerced_dataset(train_paths, train_labels, transform=transform, augment=True)\n",
    "test_dataset  = load_ucmerced_dataset(test_paths, test_labels, transform=transform, augment=False)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e639e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CNNMLP(num_classes=len(class_names))\n",
    "trainer = create_cnn_mlp_trainer(model, len(class_names), device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_mlp_classifier(trainer, train_loader, test_loader, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaca7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_model(trainer, test_loader)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f331b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = get_training_history(trainer)\n",
    "plot_training_history(history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24634d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augmented_samples(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde5269",
   "metadata": {},
   "source": [
    "### Train without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_no_aug = load_ucmerced_dataset(train_paths, train_labels, transform=transform, augment=False)\n",
    "print(len(train_dataset_no_aug), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_no_aug = DataLoader(\n",
    "    train_dataset_no_aug,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_no_aug = CNNMLP(num_classes=len(class_names))\n",
    "trainer_no_aug = create_cnn_mlp_trainer(model_no_aug, len(class_names), device)\n",
    "trainer_no_aug = train_mlp_classifier(trainer_no_aug, train_loader_no_aug, test_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55136ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_no_aug = get_training_history(trainer_no_aug)\n",
    "plot_training_history(history_no_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd52edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_no_aug = test_model(trainer_no_aug, test_loader)\n",
    "print(\"Test Accuracy:\", accuracy_no_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff53a4",
   "metadata": {},
   "source": [
    "### Train with adam instead of adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c95706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = CNNMLP(num_classes=len(class_names))\n",
    "trainer_adam = create_cnn_mlp_trainer(model_adam, len(class_names), device, opt=\"Adam\")\n",
    "trainer_adam = train_mlp_classifier(trainer_adam, train_loader, test_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = get_training_history(trainer_adam)\n",
    "plot_training_history(history_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca90388",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_adam = test_model(trainer_adam, test_loader)\n",
    "print(\"Test Accuracy:\", accuracy_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bc062",
   "metadata": {},
   "source": [
    "### Freeze last 10 convolutional layers and train again (fine tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_last_n_layers(model, n=10):\n",
    "\n",
    "    modules_with_params = [\n",
    "        m for m in model.feature_extractor.modules()\n",
    "        if any(p is not None for p in m.parameters(recurse=False))\n",
    "    ]\n",
    "\n",
    "    layers_to_unfreeze = modules_with_params[-n:]\n",
    "    for layer in layers_to_unfreeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Unfrozen last {n} layers of Xception feature extractor.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_finetune = CNNMLP(num_classes=len(class_names))\n",
    "unfreeze_last_n_layers(model_finetune, n=10)\n",
    "\n",
    "trainer_finetune = create_cnn_mlp_trainer(\n",
    "    model_finetune,\n",
    "    num_classes=len(class_names),\n",
    "    device=device,\n",
    "    opt=\"Adam\"\n",
    ")\n",
    "\n",
    "trainer_finetune[\"optimizer\"] = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_finetune.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "trainer_finetune = train_mlp_classifier(\n",
    "    trainer_finetune,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# ---- Test ----\n",
    "accuracy_finetune = test_model(trainer_finetune, test_loader)\n",
    "print(\"Fine-tuned Test Accuracy:\", accuracy_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_finetune = get_training_history(trainer_finetune)\n",
    "plot_training_history(history_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251acf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Plot final test accuracies of all trained models\n",
    "# =============================================================\n",
    "\n",
    "# Collect results\n",
    "model_names = [\n",
    "    \"Adagrad + Aug\",\n",
    "    \"Adagrad (No Aug)\",\n",
    "    \"Adam + Aug\",\n",
    "    \"Fine-tuned Adam\"\n",
    "]\n",
    "\n",
    "test_accuracies = [\n",
    "    accuracy,              # trainer (aug, adagrad)\n",
    "    accuracy_no_aug,       # trainer_no_aug\n",
    "    accuracy_adam,         # trainer_adam\n",
    "    accuracy_finetune      # trainer_finetune\n",
    "]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(model_names, test_accuracies)\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Comparison of Test Accuracy Across Models\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "for i, acc in enumerate(test_accuracies):\n",
    "    plt.text(i, acc + 1, f\"{acc:.2f}%\", ha='center')\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
